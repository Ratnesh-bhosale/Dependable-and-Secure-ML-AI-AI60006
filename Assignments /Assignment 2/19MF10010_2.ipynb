{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fy9zS0HjX1e7"
      },
      "source": [
        "# Assignment 2\n",
        "## Course - Dependable and Secure AI-ML (AI60006)\n",
        "### Name - Bhosale Ratnesh Sambhajirao (19MF10010)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1CGNmcsYHsH"
      },
      "source": [
        "## Problem 1 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiflWHnqYH1m"
      },
      "source": [
        "Take a screenshot of your outputs and record the timing required to compute the Federated Learning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwThQvW2Wchs",
        "outputId": "2c84accf-e57c-4387-e830-9cb1fd40d068"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting phe\n",
            "  Downloading phe-1.5.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: phe\n",
            "Successfully installed phe-1.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install phe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3umtNt4WRAz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_diabetes\n",
        "import phe as paillier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmmr3KoCffGK"
      },
      "outputs": [],
      "source": [
        "seed = 100\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aty0-XLofhQv"
      },
      "outputs": [],
      "source": [
        "def get_data(n_clients):\n",
        "    \"\"\"\n",
        "    Import the dataset via sklearn, shuffle and split train/test.\n",
        "    Return training, target lists for `n_clients` and a holdout test set\n",
        "    \"\"\"\n",
        "    print(\"Loading data\")\n",
        "    diabetes = load_diabetes()\n",
        "    y = diabetes.target\n",
        "    X = diabetes.data\n",
        "    # Add constant to emulate intercept\n",
        "    X = np.c_[X, np.ones(X.shape[0])]\n",
        "\n",
        "    # The features are already preprocessed\n",
        "    # Shuffle\n",
        "    perm = np.random.permutation(X.shape[0])\n",
        "    X, y = X[perm, :], y[perm]\n",
        "\n",
        "    # Select test at random\n",
        "    test_size = 50\n",
        "    test_idx = np.random.choice(X.shape[0], size=test_size, replace=False)\n",
        "    train_idx = np.ones(X.shape[0], dtype=bool)\n",
        "    train_idx[test_idx] = False\n",
        "    X_test, y_test = X[test_idx, :], y[test_idx]\n",
        "    X_train, y_train = X[train_idx, :], y[train_idx]\n",
        "\n",
        "    # Split train among multiple clients.\n",
        "    # The selection is not at random. We simulate the fact that each client\n",
        "    # sees a potentially very different sample of patients.\n",
        "    X, y = [], []\n",
        "    step = int(X_train.shape[0] / n_clients)\n",
        "    for c in range(n_clients):\n",
        "        X.append(X_train[step * c: step * (c + 1), :])\n",
        "        y.append(y_train[step * c: step * (c + 1)])\n",
        "\n",
        "    return X, y, X_test, y_test\n",
        "\n",
        "\n",
        "def mean_square_error(y_pred, y):\n",
        "    \"\"\" 1/m * \\sum_{i=1..m} (y_pred_i - y_i)^2 \"\"\"\n",
        "    return np.mean((y - y_pred) ** 2)\n",
        "\n",
        "\n",
        "def encrypt_vector(public_key, x):\n",
        "    return [public_key.encrypt(i) for i in x]\n",
        "\n",
        "\n",
        "def decrypt_vector(private_key, x):\n",
        "    return np.array([private_key.decrypt(i) for i in x])\n",
        "\n",
        "\n",
        "def sum_encrypted_vectors(x, y):\n",
        "    if len(x) != len(y):\n",
        "        raise ValueError('Encrypted vectors must have the same size')\n",
        "    return [x[i] + y[i] for i in range(len(x))]\n",
        "\n",
        "\n",
        "class Server:\n",
        "    \"\"\"Private key holder. Decrypts the average gradient\"\"\"\n",
        "\n",
        "    def __init__(self, key_length):\n",
        "         keypair = paillier.generate_paillier_keypair(n_length=key_length)\n",
        "         self.pubkey, self.privkey = keypair\n",
        "\n",
        "    def decrypt_aggregate(self, input_model, n_clients):\n",
        "        return decrypt_vector(self.privkey, input_model) / n_clients\n",
        "\n",
        "\n",
        "class Client:\n",
        "    \"\"\"Runs linear regression with local data or by gradient steps,\n",
        "    where gradient can be passed in.\n",
        "    Using public key can encrypt locally computed gradients.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name, X, y, pubkey):\n",
        "        self.name = name\n",
        "        self.pubkey = pubkey\n",
        "        self.X, self.y = X, y\n",
        "        self.weights = np.zeros(X.shape[1])\n",
        "\n",
        "    def fit(self, n_iter, eta=0.01):\n",
        "        \"\"\"Linear regression for n_iter\"\"\"\n",
        "        for _ in range(n_iter):\n",
        "            gradient = self.compute_gradient()\n",
        "            self.gradient_step(gradient, eta)\n",
        "\n",
        "    def gradient_step(self, gradient, eta=0.01):\n",
        "        \"\"\"Update the model with the given gradient\"\"\"\n",
        "        self.weights -= eta * gradient\n",
        "\n",
        "    def compute_gradient(self):\n",
        "        \"\"\"Compute the gradient of the current model using the training set\n",
        "        \"\"\"\n",
        "        delta = self.predict(self.X) - self.y\n",
        "        return delta.dot(self.X) / len(self.X)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Score test data\"\"\"\n",
        "        return X.dot(self.weights)\n",
        "\n",
        "    def encrypted_gradient(self, sum_to=None):\n",
        "        \"\"\"Compute and encrypt gradient.\n",
        "        When `sum_to` is given, sum the encrypted gradient to it, assumed\n",
        "        to be another vector of the same size\n",
        "        \"\"\"\n",
        "        gradient = self.compute_gradient()\n",
        "        encrypted_gradient = encrypt_vector(self.pubkey, gradient)\n",
        "\n",
        "        if sum_to is not None:\n",
        "            return sum_encrypted_vectors(sum_to, encrypted_gradient)\n",
        "        else:\n",
        "            return encrypted_gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3SQsvIbfk2-"
      },
      "outputs": [],
      "source": [
        "def federated_learning(X, y, X_test, y_test, config):\n",
        "    n_clients = config['n_clients']\n",
        "    n_iter = config['n_iter']\n",
        "    names = ['Hospital {}'.format(i) for i in range(1, n_clients + 1)]\n",
        "\n",
        "    # Instantiate the server and generate private and public keys\n",
        "    # NOTE: using smaller keys sizes wouldn't be cryptographically safe\n",
        "    server = Server(key_length=config['key_length'])\n",
        "\n",
        "    # Instantiate the clients.\n",
        "    # Each client gets the public key at creation and its own local dataset\n",
        "    clients = []\n",
        "    for i in range(n_clients):\n",
        "        clients.append(Client(names[i], X[i], y[i], server.pubkey))\n",
        "\n",
        "    # The federated learning with gradient descent\n",
        "    print('Running distributed gradient aggregation for {:d} iterations'\n",
        "          .format(n_iter))\n",
        "    for i in range(n_iter):\n",
        "\n",
        "        # Compute gradients, encrypt and aggregate\n",
        "        encrypt_aggr = clients[0].encrypted_gradient(sum_to=None)\n",
        "        for c in clients[1:]:\n",
        "            encrypt_aggr = c.encrypted_gradient(sum_to=encrypt_aggr)\n",
        "\n",
        "        # Send aggregate to server and decrypt it\n",
        "        aggr = server.decrypt_aggregate(encrypt_aggr, n_clients)\n",
        "\n",
        "        # Take gradient steps\n",
        "        for c in clients:\n",
        "            c.gradient_step(aggr, config['eta'])\n",
        "\n",
        "    print('Error (MSE) that each client gets after running the protocol:')\n",
        "    for c in clients:\n",
        "        y_pred = c.predict(X_test)\n",
        "        mse = mean_square_error(y_pred, y_test)\n",
        "        print('{:s}:\\t{:.2f}'.format(c.name, mse))\n",
        "\n",
        "\n",
        "def local_learning(X, y, X_test, y_test, config):\n",
        "    n_clients = config['n_clients']\n",
        "    names = ['Hospital {}'.format(i) for i in range(1, n_clients + 1)]\n",
        "\n",
        "    # Instantiate the clients.\n",
        "    # Each client gets the public key at creation and its own local dataset\n",
        "    clients = []\n",
        "    for i in range(n_clients):\n",
        "        clients.append(Client(names[i], X[i], y[i], None))\n",
        "\n",
        "    # Each client trains a linear regressor on its own data\n",
        "    print('Error (MSE) that each client gets on test set by '\n",
        "          'training only on own local data:')\n",
        "    for c in clients:\n",
        "        c.fit(config['n_iter'], config['eta'])\n",
        "        y_pred = c.predict(X_test)\n",
        "        mse = mean_square_error(y_pred, y_test)\n",
        "        print('{:s}:\\t{:.2f}'.format(c.name, mse))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAnbEAqwfoVz",
        "outputId": "41760657-acc6-4b2d-ae94-afaa16f30879"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data\n",
            "Error (MSE) that each client gets on test set by training only on own local data:\n",
            "Hospital 1:\t5156.55\n",
            "Hospital 2:\t5381.63\n",
            "Hospital 3:\t5612.22\n",
            "Hospital 4:\t4980.67\n",
            "Hospital 5:\t5128.37\n",
            "Time Taken for local training:  0.005164384841918945\n",
            "Running distributed gradient aggregation for 50 iterations\n",
            "Error (MSE) that each client gets after running the protocol:\n",
            "Hospital 1:\t5128.20\n",
            "Hospital 2:\t5128.20\n",
            "Hospital 3:\t5128.20\n",
            "Hospital 4:\t5128.20\n",
            "Hospital 5:\t5128.20\n",
            "Time Taken for federated training:  50.54999828338623\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    config = {\n",
        "        'n_clients': 5,\n",
        "        'key_length': 1024,\n",
        "        'n_iter': 50,\n",
        "        'eta': 1.5,\n",
        "    }\n",
        "    # load data, train/test split and split training data between clients\n",
        "    X, y, X_test, y_test = get_data(n_clients=config['n_clients'])\n",
        "    # first each hospital learns a model on its respective dataset for comparison.\n",
        "    start = time.time()\n",
        "    local_learning(X, y, X_test, y_test, config)\n",
        "    end = time.time()\n",
        "    print(\"Time Taken for local training: \", (end-start))\n",
        "    # and now the full glory of federated learning\n",
        "    start = time.time()\n",
        "    federated_learning(X, y, X_test, y_test, config)\n",
        "    end = time.time()\n",
        "    print(\"Time Taken for federated training: \", (end-start))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBtomv9KgjOL"
      },
      "source": [
        "## Problem 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y6nmHQ2gm8o"
      },
      "source": [
        "Following the similar adequate partial homomorphism in encryption (as discussed in the class and given in the code), implement privacy-preserving SVM assuming public model private data scenario (data is encrypted but model parameters are unencrypted):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsCplZXuLc73"
      },
      "source": [
        "### Approach 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdDPyX9ggIZ7"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os.path\n",
        "from zipfile import ZipFile\n",
        "from urllib.request import urlopen\n",
        "from contextlib import contextmanager\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import phe as paillier\n",
        "\n",
        "np.random.seed(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8u2wnz5z6eB"
      },
      "outputs": [],
      "source": [
        "# Enron spam dataset hosted by https://cloudstor.aarnet.edu.au\n",
        "url = [\n",
        "    'https://cloudstor.aarnet.edu.au/plus/index.php/s/RpHZ57z2E3BTiSQ/download',\n",
        "    'https://cloudstor.aarnet.edu.au/plus/index.php/s/QVD4Xk5Cz3UVYLp/download'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3sNatzDz9Q9"
      },
      "outputs": [],
      "source": [
        "def download_data():\n",
        "    \"\"\"Download two sets of Enron1 spam/ham e-mails if they are not here\n",
        "    We will use the first as trainset and the second as testset.\n",
        "    Return the path prefix to us to load the data from disk.\"\"\"\n",
        "\n",
        "    n_datasets = 2\n",
        "    for d in range(1, n_datasets + 1):\n",
        "        if not os.path.isdir('enron%d' % d):\n",
        "\n",
        "            URL = url[d-1]\n",
        "            print(\"Downloading %d/%d: %s\" % (d, n_datasets, URL))\n",
        "            folderzip = 'enron%d.zip' % d\n",
        "\n",
        "            with urlopen(URL) as remotedata:\n",
        "                with open(folderzip, 'wb') as z:\n",
        "                    z.write(remotedata.read())\n",
        "\n",
        "            with ZipFile(folderzip) as z:\n",
        "                z.extractall()\n",
        "            os.remove(folderzip)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc0q_rce0Esi"
      },
      "outputs": [],
      "source": [
        "def preprocess_data():\n",
        "    \"\"\"\n",
        "    Get the Enron e-mails from disk.\n",
        "    Represent them as bag-of-words.\n",
        "    Shuffle and split train/test.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Importing dataset from disk...\")\n",
        "    path = 'enron1/ham/'\n",
        "    ham1 = [open(path + f, 'r', errors='replace').read().strip(r\"\\n\")\n",
        "            for f in os.listdir(path) if os.path.isfile(path + f)]\n",
        "    path = 'enron1/spam/'\n",
        "    spam1 = [open(path + f, 'r', errors='replace').read().strip(r\"\\n\")\n",
        "             for f in os.listdir(path) if os.path.isfile(path + f)]\n",
        "    path = 'enron2/ham/'\n",
        "    ham2 = [open(path + f, 'r', errors='replace').read().strip(r\"\\n\")\n",
        "            for f in os.listdir(path) if os.path.isfile(path + f)]\n",
        "    path = 'enron2/spam/'\n",
        "    spam2 = [open(path + f, 'r', errors='replace').read().strip(r\"\\n\")\n",
        "             for f in os.listdir(path) if os.path.isfile(path + f)]\n",
        "\n",
        "    # Merge and create labels\n",
        "    emails = ham1 + spam1 + ham2 + spam2\n",
        "    y = np.array([-1] * len(ham1) + [1] * len(spam1) +\n",
        "                 [-1] * len(ham2) + [1] * len(spam2))\n",
        "\n",
        "    # Words count, keep only frequent words\n",
        "    count_vect = CountVectorizer(decode_error='replace', stop_words='english',\n",
        "                                 min_df=0.001)\n",
        "    X = count_vect.fit_transform(emails)\n",
        "\n",
        "    print('Vocabulary size: %d' % X.shape[1])\n",
        "\n",
        "    # Shuffle\n",
        "    perm = np.random.permutation(X.shape[0])\n",
        "    X, y = X[perm, :], y[perm]\n",
        "\n",
        "    # Split train and test\n",
        "    split = 500\n",
        "    X_train, X_test = X[-split:, :], X[:-split, :]\n",
        "    y_train, y_test = y[-split:], y[:-split]\n",
        "\n",
        "    print(\"Labels in trainset are {:.2f} spam : {:.2f} ham\".format(\n",
        "        np.mean(y_train == 1), np.mean(y_train == -1)))\n",
        "\n",
        "    return X_train, y_train, X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACfgGtUg0HUj",
        "outputId": "417c8cb9-0db7-411f-9816-fcbaff48f166"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Importing dataset from disk...\n",
            "Vocabulary size: 7997\n",
            "Labels in trainset are 0.29 spam : 0.71 ham\n",
            "Alice: Generating paillier keypair\n",
            "Alice: Learning spam classifier\n",
            "[elapsed time: 0.05 s]\n",
            "Classify with model in the clear -- what Alice would get having Bob's data locally\n",
            "[elapsed time: 0.96 s]\n",
            "Error 0.050\n",
            "Alice: Encrypting classifier\n",
            "[elapsed time: 145.71 s]\n",
            "Bob: Scoring with encrypted classifier\n",
            "[elapsed time: 137.01 s]\n",
            "Alice: Decrypting Bob's scores\n",
            "[elapsed time: 60.93 s]\n",
            "Error 0.050 -- this is not known to Alice, who does not possess the ground truth labels\n"
          ]
        }
      ],
      "source": [
        "@contextmanager\n",
        "def timer():\n",
        "    \"\"\"Helper for measuring runtime\"\"\"\n",
        "\n",
        "    time0 = time.perf_counter()\n",
        "    yield\n",
        "    print('[elapsed time: %.2f s]' % (time.perf_counter() - time0))\n",
        "\n",
        "\n",
        "class Alice:\n",
        "    \"\"\"\n",
        "    Trains a SVM model on plaintext data,\n",
        "    encrypts the model for remote use,\n",
        "    decrypts encrypted scores using the paillier private key.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        #self.model = LogisticRegression()\n",
        "        self.model = SVC(kernel = \"linear\")\n",
        "\n",
        "    def generate_paillier_keypair(self, n_length):\n",
        "        self.pubkey, self.privkey = \\\n",
        "            paillier.generate_paillier_keypair(n_length=n_length)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model = self.model.fit(X, y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X)\n",
        "\n",
        "    def encrypt_weights(self):\n",
        "        #coef = self.model.coef_[0, :]\n",
        "        coef = self.model.coef_.toarray()[0]\n",
        "        encrypted_weights = [self.pubkey.encrypt(coef[i])\n",
        "                             for i in range(coef.shape[0])]\n",
        "        encrypted_intercept = self.pubkey.encrypt(self.model.intercept_[0])\n",
        "        return encrypted_weights, encrypted_intercept\n",
        "\n",
        "    def decrypt_scores(self, encrypted_scores):\n",
        "        return [self.privkey.decrypt(s) for s in encrypted_scores]\n",
        "\n",
        "\n",
        "class Bob:\n",
        "    \"\"\"\n",
        "    Is given the encrypted model and the public key.\n",
        "    Scores local plaintext data with the encrypted model, but cannot decrypt\n",
        "    the scores without the private key held by Alice.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pubkey):\n",
        "        self.pubkey = pubkey\n",
        "\n",
        "    def set_weights(self, weights, intercept):\n",
        "        self.weights = weights\n",
        "        self.intercept = intercept\n",
        "\n",
        "    def encrypted_score(self, x):\n",
        "        \"\"\"Compute the score of `x` by multiplying with the encrypted model,\n",
        "        which is a vector of `paillier.EncryptedNumber`\"\"\"\n",
        "        score = self.intercept\n",
        "        _, idx = x.nonzero()\n",
        "        for i in idx:\n",
        "            score += x[0, i] * self.weights[i]\n",
        "        return score\n",
        "\n",
        "    def encrypted_evaluate(self, X):\n",
        "        return [self.encrypted_score(X[i, :]) for i in range(X.shape[0])]\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    download_data()\n",
        "    X, y, X_test, y_test = preprocess_data()\n",
        "\n",
        "    print(\"Alice: Generating paillier keypair\")\n",
        "    alice = Alice()\n",
        "    # NOTE: using smaller keys sizes wouldn't be cryptographically safe\n",
        "    alice.generate_paillier_keypair(n_length=1024)\n",
        "\n",
        "    print(\"Alice: Learning spam classifier\")\n",
        "    with timer() as t:\n",
        "        alice.fit(X, y)\n",
        "\n",
        "    print(\"Classify with model in the clear -- \"\n",
        "          \"what Alice would get having Bob's data locally\")\n",
        "    with timer() as t:\n",
        "        error = np.mean(alice.predict(X_test) != y_test)\n",
        "    print(\"Error {:.3f}\".format(error))\n",
        "\n",
        "    print(\"Alice: Encrypting classifier\")\n",
        "    with timer() as t:\n",
        "        encrypted_weights, encrypted_intercept = alice.encrypt_weights()\n",
        "\n",
        "    print(\"Bob: Scoring with encrypted classifier\")\n",
        "    bob = Bob(alice.pubkey)\n",
        "    bob.set_weights(encrypted_weights, encrypted_intercept)\n",
        "    with timer() as t:\n",
        "        encrypted_scores = bob.encrypted_evaluate(X_test)\n",
        "\n",
        "    print(\"Alice: Decrypting Bob's scores\")\n",
        "    with timer() as t:\n",
        "        scores = alice.decrypt_scores(encrypted_scores)\n",
        "    error = np.mean(np.sign(scores) != y_test)\n",
        "    print(\"Error {:.3f} -- this is not known to Alice, who does not possess \"\n",
        "          \"the ground truth labels\".format(error))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXKfCGPoMcdW"
      },
      "source": [
        "### Approach 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install phe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "445h6ovAIGeT",
        "outputId": "05c4ff11-d4b1-47ff-fc06-63a4451212a2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting phe\n",
            "  Downloading phe-1.5.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: phe\n",
            "Successfully installed phe-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import phe as paillier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Generate a random dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Paillier key pair\n",
        "public_key, private_key = paillier.generate_paillier_keypair()\n",
        "\n",
        "# Encrypt the training data and labels\n",
        "X_train_encrypted = [[public_key.encrypt(x) for x in row] for row in X_train]\n",
        "y_train_encrypted = [public_key.encrypt(label) for label in y_train]\n",
        "\n",
        "# Train the SVM model on the encrypted data\n",
        "clf = SVC(kernel=\"linear\", C=1)\n",
        "clf.fit(X_train_encrypted, y_train_encrypted)\n",
        "\n",
        "# Encrypt the testing data\n",
        "X_test_encrypted = [[public_key.encrypt(x) for x in row] for row in X_test]\n",
        "\n",
        "# Predict the labels for the encrypted testing data\n",
        "y_test_predicted_encrypted = clf.predict(X_test_encrypted)\n",
        "\n",
        "# Decrypt the predicted labels\n",
        "y_test_predicted = [private_key.decrypt(pred) for pred in y_test_predicted_encrypted]\n",
        "\n",
        "# Calculate the accuracy of the model on the testing data\n",
        "accuracy = sum(y_test_predicted == y_test) / len(y_test)\n"
      ],
      "metadata": {
        "id": "4U8LJ-9PA50B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uBHtzCWQIOM1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}